No es tema de certeza

1. Entropia de Shannon - Mido la desigualdad que me salga una de las opciones.
2. Gini Index - Mira desigualdades (desigualdad es un tipo de informacion). 
3. Entropia de Renyi - Ir sumando...

ENTROPIA DE SHANNON
	Teorema de shannon - minima longitud del codigo que necesito para transmitir la informacion. -> Viene calculada por la entropia de la distribucion de probabilidades de diferentes codigos que tengo que mandar.
	
	Usa log2 porque me dice la porque me dice cual es la potencia de la base mas alta que yo tengo.
	
	3 bits - minima representacion de un dado de 6 caras (nos quedan dos vacios)
	
Como calculo la entropia de una distribucion gaussiana (continua):
	Integral f(x) ln f(x) -> Gaussian Random Variable -> S(x) = 1/2 + 1/2 ln (2pioÂ²)
			+ varianza -> + entropia

	Principio de Maxima Entropia:
		Te dice por donde empezar -> Segun las condiciones (ej. soporte, numeros posibles ej infinitos) la distribucion de entropia es una o otra. (la del ej. es la gaussiana).
		
		
	Distribucion condicional - 
		
